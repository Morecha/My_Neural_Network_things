{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bp_fit(X, target, layer_conf, max_epoch, max_error=.1, learn_rate=.1, print_per_epoch=100):\n",
    "    np.random.seed(1)\n",
    "    nin = [np.empty(i) for i in layer_conf]\n",
    "    n = [np.empty(j + 1) if i < len(layer_conf) - 1 else np.empty(j) for i, j in enumerate(layer_conf)]\n",
    "    w = np.array([np.random.rand(layer_conf[i] + 1, layer_conf[i + 1]) for i in range(len(layer_conf) - 1)])\n",
    "    dw = [np.empty((layer_conf[i] + 1, layer_conf[i + 1])) for i in range(len(layer_conf) - 1)]\n",
    "    d = [np.empty(s) for s in layer_conf[1:]]\n",
    "    din = [np.empty(s) for s in layer_conf[1:-1]]\n",
    "    epoch = 0\n",
    "    mse = 1\n",
    "\n",
    "    for i in range(0, len(n)-1):\n",
    "        n[i][-1] = 1\n",
    "    \n",
    "    while (max_epoch == -1 or epoch < max_epoch) and mse > max_error:\n",
    "        epoch += 1\n",
    "        mse = 0\n",
    "        \n",
    "        for r in range(len(X)):\n",
    "            n[0][:-1] = X[r]\n",
    "            \n",
    "            for L in range(1, len(layer_conf)):\n",
    "                nin[L] = np.dot(n[L-1], w[L-1])\n",
    "                n[L][:len(nin[L])] = sig(nin[L])\n",
    "            e = target[r] - n[-1]\n",
    "            mse += sum(e ** 2)\n",
    "            d[-1] = e * sigd(nin[-1])\n",
    "            dw[-1] = learn_rate * d[-1] * n[-2].reshape((-1, 1))\n",
    "            \n",
    "            for L in range(len(layer_conf) - 1, 1, -1):\n",
    "                din[L-2] = np.dot(d[L-1],np.transpose(w[L-1][:-1]))\n",
    "                d[L-2] = din[L-2] * np.array(sigd(nin[L-1]))\n",
    "                dw[L-2] = (learn_rate * d[L-2]) * n[L-2].reshape((-1, 1))\n",
    "            \n",
    "            w += dw\n",
    "        \n",
    "        mse /= len(X)\n",
    "        \n",
    "        if print_per_epoch > -1 and epoch % print_per_epoch == 0: print(f'Epoch {epoch}, MSE: {mse}')\n",
    "    return w, epoch, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bp_predict(X, w):\n",
    "    n = [np.empty(len(i)) for i in w]\n",
    "    nin = [np.empty(len(i[0])) for i in w]\n",
    "    predict = []\n",
    "    \n",
    "    n.append(np.empty(len(w[-1][0])))\n",
    "    \n",
    "    for x in X:\n",
    "        n[0][:-1] = x\n",
    "        \n",
    "        for L in range(0, len(w)):\n",
    "            nin[L] = np.dot(n[L], w[L])\n",
    "            n[L + 1][:len(nin[L])] = sig(nin[L])\n",
    "    \n",
    "        predict.append(n[-1].copy())\n",
    "    \n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sig(X):\n",
    "  return [1 / (1 + np.exp(-x)) for x in X]\n",
    "def sigd(X):\n",
    "  output = []\n",
    "  for i, x in enumerate(X):\n",
    "    s = sig([x])[0]\n",
    "    output.append(s * (1 - s))\n",
    "  return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def onehot_enc(lbl, min_val=0):\n",
    "    mi = min(lbl)\n",
    "    enc = np.full((len(lbl), max(lbl) - mi + 1), min_val, np.int8)\n",
    "    for i, x in enumerate(lbl):\n",
    "        enc[i, x - mi] = 1\n",
    "    return enc\n",
    "    \n",
    "def onehot_dec(enc, mi=0):\n",
    "    return [np.argmax(e) + mi for e in enc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-58-3f04843ee786>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  w = np.array([np.random.rand(layer_conf[i] + 1, layer_conf[i + 1]) for i in range(len(layer_conf) - 1)])\n",
      "<ipython-input-58-3f04843ee786>:35: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  w += dw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, MSE: 0.19045841193641894\n",
      "Epoch 200, MSE: 0.07191039575015529\n",
      "Epoch 300, MSE: 0.05477329557645173\n",
      "Epoch 400, MSE: 0.04868920708563229\n",
      "Epoch 500, MSE: 0.04552486755428171\n",
      "Epoch 600, MSE: 0.04362905088225506\n",
      "Epoch 700, MSE: 0.04241256560334685\n",
      "Epoch 800, MSE: 0.041594650366039744\n",
      "Epoch 900, MSE: 0.04102273137630642\n",
      "Epoch 1000, MSE: 0.0406083359660748\n",
      "Epoch 1100, MSE: 0.04029801289043292\n",
      "Epoch 1200, MSE: 0.04005841386726616\n",
      "Epoch 1300, MSE: 0.03986814230065838\n",
      "Epoch 1400, MSE: 0.03971311802789583\n",
      "Epoch 1500, MSE: 0.03958385779556943\n",
      "Epoch 1600, MSE: 0.03947383561024381\n",
      "Epoch 1700, MSE: 0.03937846948604513\n",
      "Epoch 1800, MSE: 0.03929448104045514\n",
      "Epoch 1900, MSE: 0.039219482316550806\n",
      "Epoch 2000, MSE: 0.03915170409681103\n",
      "Epoch 2100, MSE: 0.03908981406378582\n",
      "Epoch 2200, MSE: 0.03903279303963392\n",
      "Epoch 2300, MSE: 0.03897984938618559\n",
      "Epoch 2400, MSE: 0.03893035885568421\n",
      "Epoch 2500, MSE: 0.0388838216480035\n",
      "Epoch 2600, MSE: 0.03883983124208961\n",
      "Epoch 2700, MSE: 0.038798051366308726\n",
      "Epoch 2800, MSE: 0.038758198636248825\n",
      "Epoch 2900, MSE: 0.03872002915213722\n",
      "Epoch 3000, MSE: 0.03868332785594714\n",
      "Epoch 3100, MSE: 0.03864789979231762\n",
      "Epoch 3200, MSE: 0.03861356265791633\n",
      "Epoch 3300, MSE: 0.038580140202724725\n",
      "Epoch 3400, MSE: 0.0385474561955684\n",
      "Epoch 3500, MSE: 0.03851532881182212\n",
      "Epoch 3600, MSE: 0.03848356546793632\n",
      "Epoch 3700, MSE: 0.038451958336899995\n",
      "Epoch 3800, MSE: 0.0384202810445068\n",
      "Epoch 3900, MSE: 0.038388287361219416\n",
      "Epoch 4000, MSE: 0.03835571301918622\n",
      "Epoch 4100, MSE: 0.03832228197957576\n",
      "Epoch 4200, MSE: 0.03828771834602241\n",
      "Epoch 4300, MSE: 0.038251764401708514\n",
      "Epoch 4400, MSE: 0.03821420374245194\n",
      "Epoch 4500, MSE: 0.03817488629202496\n",
      "Epoch 4600, MSE: 0.03813374978204781\n",
      "Epoch 4700, MSE: 0.03809083127566701\n",
      "Epoch 4800, MSE: 0.03804626373235183\n",
      "Epoch 4900, MSE: 0.03800025666485159\n",
      "Epoch 5000, MSE: 0.03795306515142552\n",
      "Epoch 5100, MSE: 0.03790495524236822\n",
      "Epoch 5200, MSE: 0.03785617416784301\n",
      "Epoch 5300, MSE: 0.037806930812666\n",
      "Epoch 5400, MSE: 0.03775738762455582\n",
      "Epoch 5500, MSE: 0.03770766169141783\n",
      "Epoch 5600, MSE: 0.037657831262923075\n",
      "Epoch 5700, MSE: 0.03760794427131331\n",
      "Epoch 5800, MSE: 0.037558026543865884\n",
      "Epoch 5900, MSE: 0.037508088594593536\n",
      "Epoch 6000, MSE: 0.03745813073277826\n",
      "Epoch 6100, MSE: 0.03740814667329785\n",
      "Epoch 6200, MSE: 0.03735812598722526\n",
      "Epoch 6300, MSE: 0.037308055725420765\n",
      "Epoch 6400, MSE: 0.03725792148063251\n",
      "Epoch 6500, MSE: 0.03720770807808619\n",
      "Epoch 6600, MSE: 0.037157400021967615\n",
      "Epoch 6700, MSE: 0.03710698178003093\n",
      "Epoch 6800, MSE: 0.03705643795842946\n",
      "Epoch 6900, MSE: 0.037005753399718906\n",
      "Epoch 7000, MSE: 0.03695491322518184\n",
      "Epoch 7100, MSE: 0.03690390283549949\n",
      "Epoch 7200, MSE: 0.036852707879493636\n",
      "Epoch 7300, MSE: 0.03680131419805001\n",
      "Epoch 7400, MSE: 0.036749707748685606\n",
      "Epoch 7500, MSE: 0.036697874515103056\n",
      "Epoch 7600, MSE: 0.036645800405260213\n",
      "Epoch 7700, MSE: 0.03659347114081847\n",
      "Epoch 7800, MSE: 0.03654087214025398\n",
      "Epoch 7900, MSE: 0.03648798839740202\n",
      "Epoch 8000, MSE: 0.036434804356730674\n",
      "Epoch 8100, MSE: 0.03638130378621848\n",
      "Epoch 8200, MSE: 0.03632746964835072\n",
      "Epoch 8300, MSE: 0.036273283969455936\n",
      "Epoch 8400, MSE: 0.03621872770738961\n",
      "Epoch 8500, MSE: 0.03616378061744558\n",
      "Epoch 8600, MSE: 0.03610842111634397\n",
      "Epoch 8700, MSE: 0.03605262614420324\n",
      "Epoch 8800, MSE: 0.03599637102457863\n",
      "Epoch 8900, MSE: 0.03593962932290006\n",
      "Epoch 9000, MSE: 0.03588237270401619\n",
      "Epoch 9100, MSE: 0.035824570790000655\n",
      "Epoch 9200, MSE: 0.03576619101991827\n",
      "Epoch 9300, MSE: 0.035707198513869436\n",
      "Epoch 9400, MSE: 0.03564755594430492\n",
      "Epoch 9500, MSE: 0.03558722341832184\n",
      "Epoch 9600, MSE: 0.03552615837537418\n",
      "Epoch 9700, MSE: 0.03546431550553893\n",
      "Epoch 9800, MSE: 0.03540164669409521\n",
      "Epoch 9900, MSE: 0.03533810099868561\n",
      "Epoch 10000, MSE: 0.03527362466563006\n",
      "Epochs: 10000, MSE: 0.03527362466563006\n",
      "Output: [0, 1, 1, 0, 2, 1, 2, 0, 0, 2, 1, 0, 2, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 2, 1, 0, 0, 1, 2, 1, 2, 1, 2, 2, 0, 1, 0, 1, 2, 2, 0, 2, 2, 1]\n",
      "True  : [0, 1, 1, 0, 2, 1, 2, 0, 0, 2, 1, 0, 2, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 2, 1, 0, 0, 1, 2, 1, 2, 1, 2, 2, 0, 1, 0, 1, 2, 2, 0, 2, 2, 1]\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = minmax_scale(iris.data)\n",
    "Y = onehot_enc(iris.target)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=.3,random_state=1)\n",
    "w, ep, mse = bp_fit(X_train, y_train, layer_conf=(4, 3, 3), learn_rate=.1, max_epoch=10000, max_error=.001, print_per_epoch=100)\n",
    "\n",
    "print(f'Epochs: {ep}, MSE: {mse}')\n",
    "\n",
    "predict = bp_predict(X_test, w)\n",
    "predict = onehot_dec(predict)\n",
    "y_test = onehot_dec(y_test)\n",
    "accuracy = accuracy_score(predict, y_test)\n",
    "\n",
    "print('Output:', predict)\n",
    "print('True  :', y_test)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-58-3f04843ee786>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  w = np.array([np.random.rand(layer_conf[i] + 1, layer_conf[i + 1]) for i in range(len(layer_conf) - 1)])\n",
      "<ipython-input-58-3f04843ee786>:35: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  w += dw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, MSE: 0.5587148548510854\n",
      "Epochs: 29, MSE: 0.4956900384526158\n",
      "Output: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "True  : [0, 1, 1, 0, 2, 1, 2, 0, 0, 2, 1, 0, 2, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 2, 1, 0, 0, 1, 2, 1, 2, 1, 2, 2, 0, 1, 0, 1, 2, 2, 0, 2, 2, 1]\n",
      "Accuracy: 0.28888888888888886\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=.3,random_state=1)\n",
    "w, ep, mse = bp_fit(X_train, y_train, layer_conf=(4, 2, 3), learn_rate=.1, max_epoch=100, max_error=.5, print_per_epoch=25)\n",
    "\n",
    "print(f'Epochs: {ep}, MSE: {mse}')\n",
    "\n",
    "predict = bp_predict(X_test, w)\n",
    "predict = onehot_dec(predict)\n",
    "y_test = onehot_dec(y_test)\n",
    "accuracy = accuracy_score(predict, y_test)\n",
    "\n",
    "print('Output:', predict)\n",
    "print('True  :', y_test)\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-58-3f04843ee786>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  w = np.array([np.random.rand(layer_conf[i] + 1, layer_conf[i + 1]) for i in range(len(layer_conf) - 1)])\n",
      "<ipython-input-58-3f04843ee786>:35: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  w += dw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, MSE: 1.9997475602526067\n",
      "Epoch 50, MSE: 1.9996839107720699\n",
      "Epoch 75, MSE: 1.9995729256252905\n",
      "Epoch 100, MSE: 1.9993234166160632\n",
      "Epoch 125, MSE: 1.998004842894972\n",
      "Epoch 150, MSE: 1.5034310711672474\n",
      "Epoch 175, MSE: 0.9882734055300789\n",
      "Epoch 200, MSE: 0.9358385629366136\n",
      "Epoch 225, MSE: 0.8962403847767554\n",
      "Epoch 250, MSE: 0.5720113088187334\n",
      "Epoch 275, MSE: 0.17189641558830474\n",
      "Epoch 300, MSE: 0.13551945448567232\n",
      "Epoch 325, MSE: 0.10976277201695424\n",
      "Epoch 350, MSE: 0.0915915464816905\n",
      "Epoch 375, MSE: 0.07929291362007071\n",
      "Epoch 400, MSE: 0.07095077966556532\n",
      "Epoch 425, MSE: 0.0651161047205166\n",
      "Epoch 450, MSE: 0.060880862438032445\n",
      "Epoch 475, MSE: 0.057698280991125867\n",
      "Epoch 500, MSE: 0.05523313404391773\n",
      "Epoch 525, MSE: 0.05327321999520484\n",
      "Epoch 550, MSE: 0.05168001906066302\n",
      "Epoch 575, MSE: 0.050360624096748914\n",
      "Epoch 600, MSE: 0.04925114278017032\n",
      "Epoch 625, MSE: 0.04830653743243756\n",
      "Epoch 650, MSE: 0.047494247901885826\n",
      "Epoch 675, MSE: 0.0467901200965703\n",
      "Epoch 700, MSE: 0.04617577407555434\n",
      "Epoch 725, MSE: 0.04563688398575028\n",
      "Epoch 750, MSE: 0.04516204040471444\n",
      "Epoch 775, MSE: 0.04474198671995559\n",
      "Epoch 800, MSE: 0.04436909700191672\n",
      "Epoch 825, MSE: 0.04403701092671843\n",
      "Epoch 850, MSE: 0.04374037193810043\n",
      "Epoch 875, MSE: 0.04347463432451932\n",
      "Epoch 900, MSE: 0.043235917238964916\n",
      "Epoch 925, MSE: 0.04302089148908972\n",
      "Epoch 950, MSE: 0.04282668984054098\n",
      "Epoch 975, MSE: 0.04265083467662365\n",
      "Epoch 1000, MSE: 0.042491178822878756\n",
      "Epoch 1025, MSE: 0.04234585660405336\n",
      "Epoch 1050, MSE: 0.04221324302029274\n",
      "Epoch 1075, MSE: 0.04209191947439561\n",
      "Epoch 1100, MSE: 0.041980644854319835\n",
      "Epoch 1125, MSE: 0.04187833103726306\n",
      "Epoch 1150, MSE: 0.04178402207201135\n",
      "Epoch 1175, MSE: 0.04169687643867347\n",
      "Epoch 1200, MSE: 0.041616151894331044\n",
      "Epoch 1225, MSE: 0.04154119249909167\n",
      "Epoch 1250, MSE: 0.041471417485760075\n",
      "Epoch 1275, MSE: 0.04140631169206682\n",
      "Epoch 1300, MSE: 0.04134541732002252\n",
      "Epoch 1325, MSE: 0.04128832682462403\n",
      "Epoch 1350, MSE: 0.041234676765381346\n",
      "Epoch 1375, MSE: 0.0411841424801585\n",
      "Epoch 1400, MSE: 0.04113643346257422\n",
      "Epoch 1425, MSE: 0.04109128934241912\n",
      "Epoch 1450, MSE: 0.04104847638383638\n",
      "Epoch 1475, MSE: 0.041007784428857004\n",
      "Epoch 1500, MSE: 0.04096902422469944\n",
      "Epoch 1525, MSE: 0.04093202508235696\n",
      "Epoch 1550, MSE: 0.04089663282169356\n",
      "Epoch 1575, MSE: 0.0408627079647713\n",
      "Epoch 1600, MSE: 0.04083012414463805\n",
      "Epoch 1625, MSE: 0.040798766701469626\n",
      "Epoch 1650, MSE: 0.040768531441920726\n",
      "Epoch 1675, MSE: 0.04073932354090521\n",
      "Epoch 1700, MSE: 0.04071105656789461\n",
      "Epoch 1725, MSE: 0.040683651622262944\n",
      "Epoch 1750, MSE: 0.04065703656430091\n",
      "Epoch 1775, MSE: 0.040631145330300766\n",
      "Epoch 1800, MSE: 0.04060591732165031\n",
      "Epoch 1825, MSE: 0.040581296859184025\n",
      "Epoch 1850, MSE: 0.040557232695172474\n",
      "Epoch 1875, MSE: 0.04053367757630276\n",
      "Epoch 1900, MSE: 0.04051058785184489\n",
      "Epoch 1925, MSE: 0.040487923121922585\n",
      "Epoch 1950, MSE: 0.04046564592144085\n",
      "Epoch 1975, MSE: 0.0404437214357632\n",
      "Epoch 2000, MSE: 0.04042211724471034\n",
      "Epoch 2025, MSE: 0.04040080309186226\n",
      "Epoch 2050, MSE: 0.040379750676506135\n",
      "Epoch 2075, MSE: 0.04035893346589019\n",
      "Epoch 2100, MSE: 0.04033832652571125\n",
      "Epoch 2125, MSE: 0.04031790636701106\n",
      "Epoch 2150, MSE: 0.04029765080786383\n",
      "Epoch 2175, MSE: 0.04027753884842163\n",
      "Epoch 2200, MSE: 0.0402575505580455\n",
      "Epoch 2225, MSE: 0.04023766697339492\n",
      "Epoch 2250, MSE: 0.04021787000646557\n",
      "Epoch 2275, MSE: 0.040198142361681295\n",
      "Epoch 2300, MSE: 0.04017846746123177\n",
      "Epoch 2325, MSE: 0.04015882937793803\n",
      "Epoch 2350, MSE: 0.04013921277498895\n",
      "Epoch 2375, MSE: 0.04011960285196157\n",
      "Epoch 2400, MSE: 0.040099985296582194\n",
      "Epoch 2425, MSE: 0.04008034624173276\n",
      "Epoch 2450, MSE: 0.04006067222724446\n",
      "Epoch 2475, MSE: 0.040040950166043816\n",
      "Epoch 2500, MSE: 0.04002116731424918\n",
      "Epoch 2525, MSE: 0.04000131124482989\n",
      "Epoch 2550, MSE: 0.03998136982445212\n",
      "Epoch 2575, MSE: 0.03996133119315695\n",
      "Epoch 2600, MSE: 0.03994118374651074\n",
      "Epoch 2625, MSE: 0.03992091611988701\n",
      "Epoch 2650, MSE: 0.03990051717453424\n",
      "Epoch 2675, MSE: 0.03987997598509452\n",
      "Epoch 2700, MSE: 0.03985928182823589\n",
      "Epoch 2725, MSE: 0.039838424172073755\n",
      "Epoch 2750, MSE: 0.039817392666053386\n",
      "Epoch 2775, MSE: 0.039796177130984725\n",
      "Epoch 2800, MSE: 0.039774767548920696\n",
      "Epoch 2825, MSE: 0.03975315405259317\n",
      "Epoch 2850, MSE: 0.03973132691413366\n",
      "Epoch 2875, MSE: 0.03970927653282525\n",
      "Epoch 2900, MSE: 0.03968699342166375\n",
      "Epoch 2925, MSE: 0.03966446819252599\n",
      "Epoch 2950, MSE: 0.03964169153978163\n",
      "Epoch 2975, MSE: 0.03961865422221429\n",
      "Epoch 3000, MSE: 0.039595347043157925\n",
      "Epoch 3025, MSE: 0.039571760828789035\n",
      "Epoch 3050, MSE: 0.039547886404556296\n",
      "Epoch 3075, MSE: 0.039523714569765694\n",
      "Epoch 3100, MSE: 0.03949923607037553\n",
      "Epoch 3125, MSE: 0.0394744415700906\n",
      "Epoch 3150, MSE: 0.039449321619874564\n",
      "Epoch 3175, MSE: 0.03942386662602585\n",
      "Epoch 3200, MSE: 0.03939806681698282\n",
      "Epoch 3225, MSE: 0.039371912209044284\n",
      "Epoch 3250, MSE: 0.03934539257119209\n",
      "Epoch 3275, MSE: 0.039318497389217284\n",
      "Epoch 3300, MSE: 0.03929121582934152\n",
      "Epoch 3325, MSE: 0.03926353670152241\n",
      "Epoch 3350, MSE: 0.0392354484226143\n",
      "Epoch 3375, MSE: 0.039206938979548134\n",
      "Epoch 3400, MSE: 0.039177995892663124\n",
      "Epoch 3425, MSE: 0.039148606179308335\n",
      "Epoch 3450, MSE: 0.039118756317803156\n",
      "Epoch 3475, MSE: 0.039088432211826174\n",
      "Epoch 3500, MSE: 0.03905761915527099\n",
      "Epoch 3525, MSE: 0.0390263017975883\n",
      "Epoch 3550, MSE: 0.03899446410961403\n",
      "Epoch 3575, MSE: 0.03896208934985942\n",
      "Epoch 3600, MSE: 0.03892916003122991\n",
      "Epoch 3625, MSE: 0.03889565788812207\n",
      "Epoch 3650, MSE: 0.03886156384384855\n",
      "Epoch 3675, MSE: 0.038826857978327585\n",
      "Epoch 3700, MSE: 0.038791519495983647\n",
      "Epoch 3725, MSE: 0.038755526693804616\n",
      "Epoch 3750, MSE: 0.038718856929513185\n",
      "Epoch 3775, MSE: 0.03868148658982151\n",
      "Epoch 3800, MSE: 0.03864339105875625\n",
      "Epoch 3825, MSE: 0.0386045446860585\n",
      "Epoch 3850, MSE: 0.03856492075569029\n",
      "Epoch 3875, MSE: 0.03852449145450279\n",
      "Epoch 3900, MSE: 0.0384832278411537\n",
      "Epoch 3925, MSE: 0.038441099815389734\n",
      "Epoch 3950, MSE: 0.03839807608785375\n",
      "Epoch 3975, MSE: 0.03835412415060237\n",
      "Epoch 4000, MSE: 0.038309210248576\n",
      "Epoch 4025, MSE: 0.03826329935229459\n",
      "Epoch 4050, MSE: 0.038216355132106465\n",
      "Epoch 4075, MSE: 0.03816833993436942\n",
      "Epoch 4100, MSE: 0.038119214759992116\n",
      "Epoch 4125, MSE: 0.03806893924582302\n",
      "Epoch 4150, MSE: 0.03801747164943695\n",
      "Epoch 4175, MSE: 0.03796476883792569\n",
      "Epoch 4200, MSE: 0.03791078628137211\n",
      "Epoch 4225, MSE: 0.03785547805174965\n",
      "Epoch 4250, MSE: 0.03779879682806261\n",
      "Epoch 4275, MSE: 0.03774069390861235\n",
      "Epoch 4300, MSE: 0.037681119231346066\n",
      "Epoch 4325, MSE: 0.037620021403318375\n",
      "Epoch 4350, MSE: 0.03755734774035915\n",
      "Epoch 4375, MSE: 0.03749304431811215\n",
      "Epoch 4400, MSE: 0.0374270560356636\n",
      "Epoch 4425, MSE: 0.037359326693033164\n",
      "Epoch 4450, MSE: 0.03728979908383724\n",
      "Epoch 4475, MSE: 0.037218415104462284\n",
      "Epoch 4500, MSE: 0.03714511588109398\n",
      "Epoch 4525, MSE: 0.03706984191593337\n",
      "Epoch 4550, MSE: 0.03699253325389567\n",
      "Epoch 4575, MSE: 0.0369131296710172\n",
      "Epoch 4600, MSE: 0.03683157088569786\n",
      "Epoch 4625, MSE: 0.03674779679376229\n",
      "Epoch 4650, MSE: 0.036661747728144525\n",
      "Epoch 4675, MSE: 0.036573364743771444\n",
      "Epoch 4700, MSE: 0.03648258992794086\n",
      "Epoch 4725, MSE: 0.03638936673615906\n",
      "Epoch 4750, MSE: 0.03629364035302414\n",
      "Epoch 4775, MSE: 0.036195358077294675\n",
      "Epoch 4800, MSE: 0.03609446972980319\n",
      "Epoch 4825, MSE: 0.03599092808233252\n",
      "Epoch 4850, MSE: 0.0358846893049958\n",
      "Epoch 4875, MSE: 0.035775713429052246\n",
      "Epoch 4900, MSE: 0.03566396482145545\n",
      "Epoch 4925, MSE: 0.035549412666799875\n",
      "Epoch 4950, MSE: 0.0354320314516967\n",
      "Epoch 4975, MSE: 0.03531180144602081\n",
      "Epoch 5000, MSE: 0.03518870917492074\n",
      "Epoch 5025, MSE: 0.0350627478750203\n",
      "Epoch 5050, MSE: 0.03493391792786627\n",
      "Epoch 5075, MSE: 0.03480222726343386\n",
      "Epoch 5100, MSE: 0.03466769172639564\n",
      "Epoch 5125, MSE: 0.03453033539792656\n",
      "Epoch 5150, MSE: 0.03439019086605146\n",
      "Epoch 5175, MSE: 0.034247299437976414\n",
      "Epoch 5200, MSE: 0.03410171128846137\n",
      "Epoch 5225, MSE: 0.033953485539103524\n",
      "Epoch 5250, MSE: 0.03380269026438359\n",
      "Epoch 5275, MSE: 0.033649402421467925\n",
      "Epoch 5300, MSE: 0.03349370770203109\n",
      "Epoch 5325, MSE: 0.03333570030572684\n",
      "Epoch 5350, MSE: 0.033175482636353906\n",
      "Epoch 5375, MSE: 0.03301316492319473\n",
      "Epoch 5400, MSE: 0.032848864771397125\n",
      "Epoch 5425, MSE: 0.03268270664658899\n",
      "Epoch 5450, MSE: 0.03251482130009802\n",
      "Epoch 5475, MSE: 0.03234534514218683\n",
      "Epoch 5500, MSE: 0.0321744195715454\n",
      "Epoch 5525, MSE: 0.032002190269903606\n",
      "Epoch 5550, MSE: 0.031828806471016005\n",
      "Epoch 5575, MSE: 0.03165442021342956\n",
      "Epoch 5600, MSE: 0.031479185586367885\n",
      "Epoch 5625, MSE: 0.031303257977778776\n",
      "Epoch 5650, MSE: 0.03112679333311027\n",
      "Epoch 5675, MSE: 0.030949947432732203\n",
      "Epoch 5700, MSE: 0.030772875195144524\n",
      "Epoch 5725, MSE: 0.030595730012229705\n",
      "Epoch 5750, MSE: 0.030418663121870634\n",
      "Epoch 5775, MSE: 0.030241823022282524\n",
      "Epoch 5800, MSE: 0.030065354931437964\n",
      "Epoch 5825, MSE: 0.02988940029402709\n",
      "Epoch 5850, MSE: 0.02971409633750164\n",
      "Epoch 5875, MSE: 0.02953957567793893\n",
      "Epoch 5900, MSE: 0.029365965975718736\n",
      "Epoch 5925, MSE: 0.029193389640365434\n",
      "Epoch 5950, MSE: 0.0290219635833468\n",
      "Epoch 5975, MSE: 0.028851799017167345\n",
      "Epoch 6000, MSE: 0.028683001298715656\n",
      "Epoch 6025, MSE: 0.02851566981454428\n",
      "Epoch 6050, MSE: 0.028349897905543094\n",
      "Epoch 6075, MSE: 0.028185772828329105\n",
      "Epoch 6100, MSE: 0.028023375750589323\n",
      "Epoch 6125, MSE: 0.027862781777578335\n",
      "Epoch 6150, MSE: 0.02770406000698471\n",
      "Epoch 6175, MSE: 0.02754727360941863\n",
      "Epoch 6200, MSE: 0.02739247993184729\n",
      "Epoch 6225, MSE: 0.027239730621391088\n",
      "Epoch 6250, MSE: 0.027089071767003883\n",
      "Epoch 6275, MSE: 0.026940544056678\n",
      "Epoch 6300, MSE: 0.026794182947935865\n",
      "Epoch 6325, MSE: 0.026650018849508756\n",
      "Epoch 6350, MSE: 0.026508077312224303\n",
      "Epoch 6375, MSE: 0.02636837922726786\n",
      "Epoch 6400, MSE: 0.026230941030105537\n",
      "Epoch 6425, MSE: 0.02609577490849104\n",
      "Epoch 6450, MSE: 0.025962889013103667\n",
      "Epoch 6475, MSE: 0.025832287669485553\n",
      "Epoch 6500, MSE: 0.025703971590066273\n",
      "Epoch 6525, MSE: 0.025577938085177908\n",
      "Epoch 6550, MSE: 0.025454181272069894\n",
      "Epoch 6575, MSE: 0.025332692281041517\n",
      "Epoch 6600, MSE: 0.02521345945790588\n",
      "Epoch 6625, MSE: 0.02509646856209807\n",
      "Epoch 6650, MSE: 0.024981702959824267\n",
      "Epoch 6675, MSE: 0.024869143811739484\n",
      "Epoch 6700, MSE: 0.024758770254715762\n",
      "Epoch 6725, MSE: 0.024650559577340678\n",
      "Epoch 6750, MSE: 0.02454448738885344\n",
      "Epoch 6775, MSE: 0.0244405277812907\n",
      "Epoch 6800, MSE: 0.024338653484675635\n",
      "Epoch 6825, MSE: 0.024238836015134782\n",
      "Epoch 6850, MSE: 0.0241410458158819\n",
      "Epoch 6875, MSE: 0.02404525239104887\n",
      "Epoch 6900, MSE: 0.02395142443239063\n",
      "Epoch 6925, MSE: 0.023859529938920883\n",
      "Epoch 6950, MSE: 0.02376953632957518\n",
      "Epoch 6975, MSE: 0.023681410549021946\n",
      "Epoch 7000, MSE: 0.023595119166769384\n",
      "Epoch 7025, MSE: 0.023510628469738098\n",
      "Epoch 7050, MSE: 0.02342790454848668\n",
      "Epoch 7075, MSE: 0.023346913377293613\n",
      "Epoch 7100, MSE: 0.023267620888311046\n",
      "Epoch 7125, MSE: 0.023189993040016177\n",
      "Epoch 7150, MSE: 0.023113995880192775\n",
      "Epoch 7175, MSE: 0.023039595603681604\n",
      "Epoch 7200, MSE: 0.022966758605140317\n",
      "Epoch 7225, MSE: 0.022895451527055743\n",
      "Epoch 7250, MSE: 0.02282564130325131\n",
      "Epoch 7275, MSE: 0.022757295198129458\n",
      "Epoch 7300, MSE: 0.022690380841887202\n",
      "Epoch 7325, MSE: 0.022624866261938097\n",
      "Epoch 7350, MSE: 0.02256071991076881\n",
      "Epoch 7375, MSE: 0.022497910690453257\n",
      "Epoch 7400, MSE: 0.022436407974039493\n",
      "Epoch 7425, MSE: 0.022376181624019352\n",
      "Epoch 7450, MSE: 0.0223172020080804\n",
      "Epoch 7475, MSE: 0.022259440012335405\n",
      "Epoch 7500, MSE: 0.022202867052213346\n",
      "Epoch 7525, MSE: 0.022147455081188816\n",
      "Epoch 7550, MSE: 0.02209317659752031\n",
      "Epoch 7575, MSE: 0.022040004649154395\n",
      "Epoch 7600, MSE: 0.021987912836951104\n",
      "Epoch 7625, MSE: 0.02193687531637173\n",
      "Epoch 7650, MSE: 0.021886866797766077\n",
      "Epoch 7675, MSE: 0.02183786254538569\n",
      "Epoch 7700, MSE: 0.021789838375244026\n",
      "Epoch 7725, MSE: 0.02174277065193541\n",
      "Epoch 7750, MSE: 0.02169663628451817\n",
      "Epoch 7775, MSE: 0.02165141272156007\n",
      "Epoch 7800, MSE: 0.021607077945437855\n",
      "Epoch 7825, MSE: 0.0215636104659759\n",
      "Epoch 7850, MSE: 0.021520989313502797\n",
      "Epoch 7875, MSE: 0.02147919403139966\n",
      "Epoch 7900, MSE: 0.021438204668207356\n",
      "Epoch 7925, MSE: 0.021398001769354965\n",
      "Epoch 7950, MSE: 0.021358566368568154\n",
      "Epoch 7975, MSE: 0.021319879979008565\n",
      "Epoch 8000, MSE: 0.021281924584193954\n",
      "Epoch 8025, MSE: 0.02124468262874345\n",
      "Epoch 8050, MSE: 0.02120813700898706\n",
      "Epoch 8075, MSE: 0.021172271063478312\n",
      "Epoch 8100, MSE: 0.021137068563441528\n",
      "Epoch 8125, MSE: 0.02110251370318509\n",
      "Epoch 8150, MSE: 0.02106859109050865\n",
      "Epoch 8175, MSE: 0.021035285737126586\n",
      "Epoch 8200, MSE: 0.02100258304913302\n",
      "Epoch 8225, MSE: 0.020970468817525772\n",
      "Epoch 8250, MSE: 0.020938929208807185\n",
      "Epoch 8275, MSE: 0.020907950755678743\n",
      "Epoch 8300, MSE: 0.020877520347841894\n",
      "Epoch 8325, MSE: 0.020847625222917013\n",
      "Epoch 8350, MSE: 0.020818252957492244\n",
      "Epoch 8375, MSE: 0.020789391458310606\n",
      "Epoch 8400, MSE: 0.020761028953602063\n",
      "Epoch 8425, MSE: 0.020733153984569288\n",
      "Epoch 8450, MSE: 0.020705755397030193\n",
      "Epoch 8475, MSE: 0.02067882233322399\n",
      "Epoch 8500, MSE: 0.020652344223781965\n",
      "Epoch 8525, MSE: 0.02062631077986787\n",
      "Epoch 8550, MSE: 0.020600711985488348\n",
      "Epoch 8575, MSE: 0.02057553808997576\n",
      "Epoch 8600, MSE: 0.020550779600643117\n",
      "Epoch 8625, MSE: 0.020526427275612203\n",
      "Epoch 8650, MSE: 0.0205024721168145\n",
      "Epoch 8675, MSE: 0.020478905363163057\n",
      "Epoch 8700, MSE: 0.02045571848389595\n",
      "Epoch 8725, MSE: 0.020432903172088276\n",
      "Epoch 8750, MSE: 0.020410451338331963\n",
      "Epoch 8775, MSE: 0.02038835510458089\n",
      "Epoch 8800, MSE: 0.02036660679815908\n",
      "Epoch 8825, MSE: 0.02034519894592988\n",
      "Epoch 8850, MSE: 0.020324124268623036\n",
      "Epoch 8875, MSE: 0.020303375675316995\n",
      "Epoch 8900, MSE: 0.020282946258074613\n",
      "Epoch 8925, MSE: 0.020262829286727128\n",
      "Epoch 8950, MSE: 0.02024301820380532\n",
      "Epoch 8975, MSE: 0.020223506619614317\n",
      "Epoch 9000, MSE: 0.020204288307448044\n",
      "Epoch 9025, MSE: 0.020185357198940955\n",
      "Epoch 9050, MSE: 0.020166707379553908\n",
      "Epoch 9075, MSE: 0.02014833308419003\n",
      "Epoch 9100, MSE: 0.02013022869293867\n",
      "Epoch 9125, MSE: 0.02011238872694329\n",
      "Epoch 9150, MSE: 0.020094807844390297\n",
      "Epoch 9175, MSE: 0.020077480836615938\n",
      "Epoch 9200, MSE: 0.02006040262432797\n",
      "Epoch 9225, MSE: 0.020043568253939017\n",
      "Epoch 9250, MSE: 0.020026972894008444\n",
      "Epoch 9275, MSE: 0.02001061183179024\n",
      "Epoch 9300, MSE: 0.019994480469883\n",
      "Epoch 9325, MSE: 0.019978574322980405\n",
      "Epoch 9350, MSE: 0.01996288901471824\n",
      "Epoch 9375, MSE: 0.01994742027461589\n",
      "Epoch 9400, MSE: 0.019932163935108785\n",
      "Epoch 9425, MSE: 0.01991711592867039\n",
      "Epoch 9450, MSE: 0.019902272285019847\n",
      "Epoch 9475, MSE: 0.019887629128413454\n",
      "Epoch 9500, MSE: 0.019873182675017264\n",
      "Epoch 9525, MSE: 0.019858929230358285\n",
      "Epoch 9550, MSE: 0.01984486518685259\n",
      "Epoch 9575, MSE: 0.019830987021406895\n",
      "Epoch 9600, MSE: 0.01981729129309217\n",
      "Epoch 9625, MSE: 0.019803774640887045\n",
      "Epoch 9650, MSE: 0.019790433781488585\n",
      "Epoch 9675, MSE: 0.019777265507188765\n",
      "Epoch 9700, MSE: 0.019764266683813956\n",
      "Epoch 9725, MSE: 0.019751434248726683\n",
      "Epoch 9750, MSE: 0.019738765208886004\n",
      "Epoch 9775, MSE: 0.019726256638966745\n",
      "Epoch 9800, MSE: 0.019713905679533888\n",
      "Epoch 9825, MSE: 0.019701709535271734\n",
      "Epoch 9850, MSE: 0.019689665473265415\n",
      "Epoch 9875, MSE: 0.019677770821333762\n",
      "Epoch 9900, MSE: 0.019666022966410655\n",
      "Epoch 9925, MSE: 0.01965441935297552\n",
      "Epoch 9950, MSE: 0.01964295748152898\n",
      "Epoch 9975, MSE: 0.019631634907114445\n",
      "Epoch 10000, MSE: 0.019620449237882327\n",
      "Epochs: 10000, MSE: 0.019620449237882327\n",
      "Output: [0, 1, 1, 0, 2, 1, 2, 0, 0, 2, 1, 0, 2, 1, 1, 0, 1, 1, 0, 0, 1, 1, 2, 0, 2, 1, 0, 0, 1, 2, 1, 2, 1, 2, 2, 0, 1, 0, 1, 2, 2, 0, 1, 2, 1]\n",
      "True  : [0, 1, 1, 0, 2, 1, 2, 0, 0, 2, 1, 0, 2, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 2, 1, 0, 0, 1, 2, 1, 2, 1, 2, 2, 0, 1, 0, 1, 2, 2, 0, 2, 2, 1]\n",
      "Accuracy: 0.9555555555555556\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=.3,random_state=1)\n",
    "w, ep, mse = bp_fit(X_train, y_train, layer_conf=(4, 25, 3), learn_rate=.1, max_epoch=10000, max_error=.01, print_per_epoch=25)\n",
    "\n",
    "print(f'Epochs: {ep}, MSE: {mse}')\n",
    "\n",
    "predict = bp_predict(X_test, w)\n",
    "predict = onehot_dec(predict)\n",
    "y_test = onehot_dec(y_test)\n",
    "accuracy = accuracy_score(predict, y_test)\n",
    "\n",
    "print('Output:', predict)\n",
    "print('True  :', y_test)\n",
    "print('Accuracy:', accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
